{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nemotron Phishing Detection Workshop\n",
    "\n",
    "This notebook walks through the full fine-tuning workflow on the Enron dataset:\n",
    "1. Download the dataset\n",
    "2. Convert to JSONL\n",
    "3. Fine-tune Nemotron with LoRA\n",
    "4. Evaluate the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f33ff6",
   "metadata": {},
   "source": [
    "## Environment setup (run in terminal)\n",
    "Create a virtual environment and install dependencies from the shell.\n",
    "\n",
    "```bash\n",
    "python3 -m venv .venv\n",
    "source .venv/bin/activate\n",
    "python -m pip install --upgrade pip\n",
    "pip install -r requirements.txt\n",
    "pip install -r requirements-vllm.txt\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aac4ffd",
   "metadata": {},
   "source": [
    "## Prepare the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e705b7",
   "metadata": {},
   "source": [
    "## Configure Kaggle API\n",
    "Export your Kaggle credentials before downloading the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea81ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KAGGLE_USERNAME'] = 'YYYYYYYYYY'\n",
    "os.environ['KAGGLE_KEY'] = 'XXXXXXXXXXXXX'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c06e40c",
   "metadata": {},
   "source": [
    "## Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5a63a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading wcukierski/enron-email-dataset to ../data/raw...\n",
      "Dataset URL: https://www.kaggle.com/datasets/wcukierski/enron-email-dataset\n",
      "Download completed, but maildir was not found. Check the output directory.\n"
     ]
    }
   ],
   "source": [
    "!python ../scripts/download_dataset.py --output_dir ../data/raw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bcf999",
   "metadata": {},
   "source": [
    "## Convert to JSONL\n",
    "This uses a simple keyword heuristic to label phishing vs benign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d065cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing emails: 517401it [05:59, 1440.15it/s]\n",
      "Wrote JSONL files to ../data/processed\n"
     ]
    }
   ],
   "source": [
    "!python ../scripts/prepare_jsonl.py --input_csv ../data/raw/emails.csv --output_dir ../data/processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e597600",
   "metadata": {},
   "source": [
    "## Inspect dataset stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ed04f8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total': 50000,\n",
       " 'train': 40000,\n",
       " 'val': 5000,\n",
       " 'test': 5000,\n",
       " 'phishing': 6567,\n",
       " 'benign': 43433}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "stats = json.loads(Path('../data/processed/stats.json').read_text())\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0fdcd8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total': 5000,\n",
       " 'train': 4000,\n",
       " 'val': 500,\n",
       " 'test': 500,\n",
       " 'phishing': 649,\n",
       " 'benign': 4351}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trim dataset to 10% to target ~1 hour training on L4\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "src_dir = Path('../data/processed')\n",
    "dst_dir = Path('../data/processed_small')\n",
    "dst_dir.mkdir(parents=True, exist_ok=True)\n",
    "seed = 42\n",
    "fraction = 0.1\n",
    "\n",
    "def sample_jsonl(src, dst, fraction, seed):\n",
    "    lines = Path(src).read_text().splitlines()\n",
    "    rng = random.Random(seed)\n",
    "    k = max(1, int(len(lines) * fraction))\n",
    "    sample = rng.sample(lines, k)\n",
    "    Path(dst).write_text('\\n'.join(sample) + '\\n')\n",
    "    return k\n",
    "\n",
    "counts = {}\n",
    "counts['train'] = sample_jsonl(src_dir / 'train.jsonl', dst_dir / 'train.jsonl', fraction, seed)\n",
    "counts['val'] = sample_jsonl(src_dir / 'val.jsonl', dst_dir / 'val.jsonl', fraction, seed)\n",
    "counts['test'] = sample_jsonl(src_dir / 'test.jsonl', dst_dir / 'test.jsonl', fraction, seed)\n",
    "\n",
    "def count_labels(path):\n",
    "    stats = {'phishing': 0, 'benign': 0, 'total': 0}\n",
    "    for line in Path(path).read_text().splitlines():\n",
    "        if not line:\n",
    "            continue\n",
    "        obj = json.loads(line)\n",
    "        label = obj.get('label', '')\n",
    "        if label in stats:\n",
    "            stats[label] += 1\n",
    "        stats['total'] += 1\n",
    "    return stats\n",
    "\n",
    "train_stats = count_labels(dst_dir / 'train.jsonl')\n",
    "val_stats = count_labels(dst_dir / 'val.jsonl')\n",
    "test_stats = count_labels(dst_dir / 'test.jsonl')\n",
    "small_stats = {\n",
    "    'total': train_stats['total'] + val_stats['total'] + test_stats['total'],\n",
    "    'train': train_stats['total'],\n",
    "    'val': val_stats['total'],\n",
    "    'test': test_stats['total'],\n",
    "    'phishing': train_stats['phishing'] + val_stats['phishing'] + test_stats['phishing'],\n",
    "    'benign': train_stats['benign'] + val_stats['benign'] + test_stats['benign'],\n",
    "}\n",
    "(dst_dir / 'stats.json').write_text(json.dumps(small_stats, indent=2))\n",
    "small_stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6208dc",
   "metadata": {},
   "source": [
    "## Base model experience\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3e8fe0",
   "metadata": {},
   "source": [
    "## Serve the base model (run in terminal)\n",
    "Start the non-tuned model before preparing data to sanity check inference.\n",
    "\n",
    "Terminal A (serve):\n",
    "```bash\n",
    "# Optional: pip install -r requirements-vllm.txt\n",
    "python scripts/serve_vllm.py --model_name nvidia/Nemotron-Mini-4B-Instruct \\\n",
    "  --served_model_name base --port 8000 \\\n",
    "  --enforce_eager --max_model_len 512 --gpu_memory_utilization 0.6 \\\n",
    "  --max_num_batched_tokens 512 --max_num_seqs 4\n",
    "```\n",
    "The base model is registered as the OpenAI model name `base`.\n",
    "\n",
    "Stop the server with Ctrl+C when done.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875a7f63",
   "metadata": {},
   "source": [
    "## Smoke test the base model (streaming)\n",
    "This example is labeled benign in the dataset and is a known base-model miss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "394ac08c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: Pipeline 30 Years\n",
      "ph"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ishing\n",
      "Predicted: phishing | Expected: benign | wrong\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "endpoint = \"http://127.0.0.1:8000/v1/completions\"\n",
    "model = \"base\"\n",
    "\n",
    "def build_prompt(subject, body):\n",
    "    return (\n",
    "        \"### Instruction:\\n\"\n",
    "        \"Classify the email as phishing or benign. Reply with only the label.\\n\"\n",
    "        \"### Email:\\n\"\n",
    "        f\"Subject: {subject.strip()}\\n\"\n",
    "        f\"Body: {body.strip()}\\n\"\n",
    "        \"### Response:\\n\"\n",
    "    )\n",
    "\n",
    "def normalize_label(text):\n",
    "    lowered = text.lower()\n",
    "    if \"phish\" in lowered:\n",
    "        return \"phishing\"\n",
    "    if \"benign\" in lowered or \"ham\" in lowered:\n",
    "        return \"benign\"\n",
    "    return \"unknown\"\n",
    "\n",
    "def stream_completion(prompt):\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": 6,\n",
    "        \"temperature\": 0.0,\n",
    "        \"stream\": True,\n",
    "    }\n",
    "    chunks = []\n",
    "    with requests.post(endpoint, json=payload, stream=True, timeout=60) as resp:\n",
    "        resp.raise_for_status()\n",
    "        for line in resp.iter_lines(decode_unicode=True):\n",
    "            if not line:\n",
    "                continue\n",
    "            if line.startswith(\"data: \"):\n",
    "                data = line[len(\"data: \"):].strip()\n",
    "                if data == \"[DONE]\":\n",
    "                    break\n",
    "                chunk = json.loads(data)\n",
    "                text = chunk.get(\"choices\", [{}])[0].get(\"text\", \"\")\n",
    "                if text:\n",
    "                    print(text, end=\"\", flush=True)\n",
    "                    chunks.append(text)\n",
    "    print()\n",
    "    return \"\".join(chunks).strip()\n",
    "\n",
    "subject = \"Pipeline 30 Years\"\n",
    "body = \"Pipeline 30 Years\"\n",
    "expected = \"benign\"\n",
    "\n",
    "print(f\"Subject: {subject}\")\n",
    "raw = stream_completion(build_prompt(subject, body))\n",
    "pred = normalize_label(raw)\n",
    "status = \"correct\" if pred == expected else \"wrong\"\n",
    "print(f\"Predicted: {pred} | Expected: {expected} | {status}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea7383e",
   "metadata": {},
   "source": [
    "## Evaluate the base model (run in terminal)\n",
    "Score 500 test samples against the base endpoint and save results. Overlength samples are skipped.\n",
    "\n",
    "Terminal B (evaluate):\n",
    "```bash\n",
    "python scripts/test_model.py --api openai \\\n",
    "  --endpoint http://127.0.0.1:8000/v1/completions \\\n",
    "  --openai_model base \\\n",
    "  --test_file data/processed_small/test.jsonl \\\n",
    "  --max_samples 500 \\\n",
    "  --output_file outputs/eval_base.json\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0360d7a",
   "metadata": {},
   "source": [
    "## View base accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f109dcf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base accuracy: 59.08% (218/369)\n",
      "Skipped (overlength): 131\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "base = json.loads(Path(\"../outputs/eval_base.json\").read_text())\n",
    "print(f\"Base accuracy: {base['accuracy']:.2%} ({base['correct']}/{base['total']})\")\n",
    "print(f\"Skipped (overlength): {base.get('skipped', 0)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156a8dc9",
   "metadata": {},
   "source": [
    "## Fine-tune and evaluate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e72cc3c",
   "metadata": {},
   "source": [
    "## Fine-tune the model (run in terminal)\n",
    "Training can take hours, so run it from a shell instead of the notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6065361",
   "metadata": {},
   "source": [
    "```bash\n",
    "python scripts/train.py --data_dir data/processed_small --output_dir outputs \\\n",
    "  --model_name nvidia/Nemotron-Mini-4B-Instruct --num_train_epochs 1 --max_seq_length 512\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb7f253",
   "metadata": {},
   "source": [
    "## Serve the tuned model (run in terminal)\n",
    "If you already trained an adapter in `outputs/adapter`, serve it with the OpenAI-compatible vLLM server so streaming works.\n",
    "\n",
    "Terminal A (serve):\n",
    "```bash\n",
    "# Optional: pip install -r requirements-vllm.txt\n",
    "python scripts/serve_vllm.py --model_name nvidia/Nemotron-Mini-4B-Instruct \\\n",
    "  --adapter_dir outputs/adapter --port 8000\n",
    "```\n",
    "The adapter is registered as the OpenAI model name `phishing`.\n",
    "\n",
    "\n",
    "If you hit CUDA OOM during startup, retry with tighter limits:\n",
    "```bash\n",
    "python scripts/serve_vllm.py --model_name nvidia/Nemotron-Mini-4B-Instruct \\\n",
    "  --adapter_dir outputs/adapter --port 8000 \\\n",
    "  --enforce_eager --max_model_len 1024 --gpu_memory_utilization 0.8 \\\n",
    "  --max_num_batched_tokens 1024 --max_num_seqs 8\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26314629",
   "metadata": {},
   "source": [
    "## Smoke test the tuned model (streaming)\n",
    "Re-run the same example to see whether the tuned model fixes the mistake.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "68fbdf2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: Pipeline 30 Years\n",
      "benign\n",
      "Predicted: benign | Expected: benign | correct\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "endpoint = \"http://127.0.0.1:8000/v1/completions\"\n",
    "model = \"phishing\"\n",
    "\n",
    "def build_prompt(subject, body):\n",
    "    return (\n",
    "        \"### Instruction:\\n\"\n",
    "        \"Classify the email as phishing or benign. Reply with only the label.\\n\"\n",
    "        \"### Email:\\n\"\n",
    "        f\"Subject: {subject.strip()}\\n\"\n",
    "        f\"Body: {body.strip()}\\n\"\n",
    "        \"### Response:\\n\"\n",
    "    )\n",
    "\n",
    "def normalize_label(text):\n",
    "    lowered = text.lower()\n",
    "    if \"phish\" in lowered:\n",
    "        return \"phishing\"\n",
    "    if \"benign\" in lowered or \"ham\" in lowered:\n",
    "        return \"benign\"\n",
    "    return \"unknown\"\n",
    "\n",
    "def stream_completion(prompt):\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": 6,\n",
    "        \"temperature\": 0.0,\n",
    "        \"stream\": True,\n",
    "    }\n",
    "    chunks = []\n",
    "    with requests.post(endpoint, json=payload, stream=True, timeout=60) as resp:\n",
    "        resp.raise_for_status()\n",
    "        for line in resp.iter_lines(decode_unicode=True):\n",
    "            if not line:\n",
    "                continue\n",
    "            if line.startswith(\"data: \"):\n",
    "                data = line[len(\"data: \"):].strip()\n",
    "                if data == \"[DONE]\":\n",
    "                    break\n",
    "                chunk = json.loads(data)\n",
    "                text = chunk.get(\"choices\", [{}])[0].get(\"text\", \"\")\n",
    "                if text:\n",
    "                    print(text, end=\"\", flush=True)\n",
    "                    chunks.append(text)\n",
    "    print()\n",
    "    return \"\".join(chunks).strip()\n",
    "\n",
    "subject = \"Pipeline 30 Years\"\n",
    "body = \"Pipeline 30 Years\"\n",
    "expected = \"benign\"\n",
    "\n",
    "print(f\"Subject: {subject}\")\n",
    "raw = stream_completion(build_prompt(subject, body))\n",
    "pred = normalize_label(raw)\n",
    "status = \"correct\" if pred == expected else \"wrong\"\n",
    "print(f\"Predicted: {pred} | Expected: {expected} | {status}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caee00a2",
   "metadata": {},
   "source": [
    "## Evaluate the tuned model (run in terminal)\n",
    "Score 500 test samples against the tuned endpoint and save results. Overlength samples are skipped.\n",
    "\n",
    "Terminal B (evaluate):\n",
    "```bash\n",
    "python scripts/test_model.py --api openai \\\n",
    "  --endpoint http://127.0.0.1:8000/v1/completions \\\n",
    "  --openai_model phishing \\\n",
    "  --test_file data/processed_small/test.jsonl \\\n",
    "  --max_samples 500 \\\n",
    "  --output_file outputs/eval_tuned.json\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11feaddf",
   "metadata": {},
   "source": [
    "## View tuned accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "486e835c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned accuracy: 87.15% (407/467)\n",
      "Skipped (overlength): 33\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "tuned = json.loads(Path(\"../outputs/eval_tuned.json\").read_text())\n",
    "print(f\"Tuned accuracy: {tuned['accuracy']:.2%} ({tuned['correct']}/{tuned['total']})\")\n",
    "print(f\"Skipped (overlength): {tuned.get('skipped', 0)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e724da2",
   "metadata": {},
   "source": [
    "## Compare results\n",
    "Calculate the accuracy gain from fine-tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8aa31152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base accuracy: 59.08% (218/369)\n",
      "Tuned accuracy: 87.15% (407/467)\n",
      "Absolute gain: 28.07 pp\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "base = json.loads(Path(\"../outputs/eval_base.json\").read_text())\n",
    "tuned = json.loads(Path(\"../outputs/eval_tuned.json\").read_text())\n",
    "\n",
    "def fmt(result):\n",
    "    return f\"{result['accuracy']:.2%} ({result['correct']}/{result['total']})\"\n",
    "\n",
    "print(\"Base accuracy:\", fmt(base))\n",
    "print(\"Tuned accuracy:\", fmt(tuned))\n",
    "print(\"Absolute gain:\", f\"{(tuned['accuracy'] - base['accuracy']) * 100:.2f} pp\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
